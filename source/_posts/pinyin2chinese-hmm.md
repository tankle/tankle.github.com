---
title: 隐马尔科夫模型实战之拼音转汉字
date: 2018-01-09 18:34:55
tags:
- HMM
- 隐马尔科夫模型
categories:
- 技术
- 机器学习
---

## 背景
    中文输入法有许多不同种的方法，主要有三大类：拼音输入法、形码输入法(如：五笔)和音型码输入法(如：二笔)，而当今最流行的输入法非拼音输入法莫属了。
    在拼音输入法中，用户首先将中文转换为拼音，然后输入法根据输入的拼音序列转换为对应的汉字，已完成中文的输入。
    在云音乐搜索中，由于用户每输入一个字符就会触发一次搜索，因此在后台能收到大量的拼音序列的搜索词。比如：zhou jie lun（周杰伦），li rong hao（李荣浩）。如果能够准确识别这些拼音序列，并转换为对应的中文搜索词，能显著提高用户的搜索体验。

## 难点
拼音转汉字的一些难点：
1. 一个拼音对应多个汉字，如何确定一个拼音对应哪个汉字？
2. 拼音词组对应多个汉字词组，比如：jie lun，有：杰伦、结论，该选择哪个转换结果呢？

## 方法
    用户输入的拼音序列，是一个可以观察到的序列，而拼音对应的汉字则可以看做是一个不可见序列，通过观察变量求解隐藏变量，这是隐马尔科夫模型的强项啊！

### 隐马尔科夫模型简介
一些符号参数说明
Q  是所有可能的状态的集合, 其中 N 是可能的状态数;
V 是所有可能的观测的集合, 其中 M 是可能的观测数;
Q={q1,q2,⋯,qN},V={v1,v2,⋯,vM}Q={q1,q2,⋯,qN},V={v1,v2,⋯,vM}
I 是长度为 T 的状态序列; O 是对应的观测序列:
I={i1,i2,⋯,iT},O={o1,o2,⋯,oT}

隐马尔科夫模型（HMM，Hidden Markov Models）可以使用一个三元组来刻画$\lambda = （pi,A,B)$:
$$\pi$$为初始化概率，，。
* $A$为状态转移概率分布
    $A = [a_i_j]_N*N$, 其中,  aijaij  是在时刻  tt  处于状态  qiqi  的条件下时刻  t+1t+1  转移到状态  qjqj  的概率:
* $B$为观测概率分布  
其中,  bj(k)bj(k)  是在时刻  tt  处于状态  qiqi  的条件下生成观测  vkvk  的概率:
* π  是初始状态概率向量:
π=(πi)N×1  

### HMM的三个基本问题

概率计算问题: 前向-后向算法——动态规划
给定模型  λ=(A,B,π)λ=(A,B,π)  和观测序列  O={o1,o2,⋯,oT}O={o1,o2,⋯,oT}
计算模型  λλ  下观测序列  OO  出现的概率  P(O∣λ)P(O∣λ)
学习问题: Baum-Welch算法(状态未知)——EM
已知观测序列 O={o1,o2,⋯,oT}O={o1,o2,⋯,oT}
估计模型  λ=(A,B,π)λ=(A,B,π)  的参数,使得在该模型下观测序列  P(O∣λ)P(O∣λ)  最大
预测问题: Viterbi算法——动态规划
已知模型  λ=(A,B,π)λ=(A,B,π) , 和观测序列  O={o1,o2,⋯,oT}O={o1,o2,⋯,oT}
求给定观测序列条件概率  P(I∣O,λ)P(I∣O,λ)  最大的状态序列  I={i1,i2,⋯,iT}


## 参考
* [Github 代码](https://github.com/tankle/NLPAlgorithm/blob/master/SpellCorrect/src/main/io/github/tankle/algorithm/DTSpellCorrectFactory.java)
*[HMM学习最佳范例四：隐马尔科夫模型] (http://www.52nlp.cn/hmm-learn-best-practices-four-hidden-markov-models)
